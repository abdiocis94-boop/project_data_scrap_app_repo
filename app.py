# app.py - Application Streamlit pour le scraping de CoinAfrique

import streamlit as st
import pandas as pd
from requests import get
from bs4 import BeautifulSoup as bs 
import os
import time
from datetime import datetime
import base64

# Configuration de la page
st.set_page_config(
    page_title="CoinAfrique Scraper",
    page_icon="üõí",
    layout="wide"
)

# Titre de l'application
st.title("üõí CoinAfrique Scraper - S√©n√©gal")
st.markdown("---")

# Fonction de scraping
@st.cache_data(show_spinner=False)
def scraping(url, stop, progress_bar=None):
    """
    Fonction pour scraper les donn√©es de CoinAfrique
    """
    df = pd.DataFrame()
    total_pages = stop
    
    for index_page in range(1, stop+1):
        # Mise √† jour de la barre de progression
        if progress_bar:
            progress_bar.progress(index_page / total_pages, 
                                 text=f"Page {index_page}/{total_pages}")
        
        url_page = f'{url}?page={index_page}'
        try:
            res = get(url_page, timeout=10)
            soup = bs(res.content, 'html.parser')
            containers = soup.find_all('div', 'col s6 m4 l3')
            data = []
            
            for container in containers:
                try:
                    type_habit = container.find('p', 'ad__card-description').a.text
                    prix = container.find('p', 'ad__card-price').a.text.strip('CFA')
                    adresse = container.find('p', 'ad__card-location').span.text
                    image = container.find('img', 'ad__card-img')['src']
                    
                    dic = {
                        "type": type_habit, 
                        "prix": prix,
                        "adresse": adresse, 
                        "image": image,
                        "date_scraping": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    data.append(dic)
                except Exception as e:
                    continue
            
            if data:
                DF = pd.DataFrame(data)
                df = pd.concat([df, DF], axis=0).reset_index(drop=True)
                
        except Exception as e:
            st.warning(f"Erreur sur la page {index_page}: {str(e)}")
            continue
    
    return df

# Fonction pour cr√©er un lien de t√©l√©chargement
def get_csv_download_link(df, filename, text="üì• T√©l√©charger CSV"):
    """
    G√©n√®re un lien de t√©l√©chargement pour un DataFrame
    """
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}" style="\
        display: inline-block;\
        padding: 0.5rem 1rem;\
        background-color: #4CAF50;\
        color: white;\
        text-decoration: none;\
        border-radius: 5px;\
        font-weight: bold;\
        margin: 5px;">{text}</a>'
    return href

# Barre lat√©rale pour les param√®tres
with st.sidebar:
    st.header("‚öôÔ∏è Param√®tres de scraping")
    st.markdown("---")
    
    # S√©lection du nombre de pages
    pages = st.slider("Nombre de pages √† scraper", 1, 20, 5, 
                     help="Plus de pages = plus de donn√©es mais temps de traitement plus long")
    
    # Bouton pour d√©marrer le scraping
    start_scraping = st.button("üöÄ Lancer le scraping", 
                              type="primary", 
                              use_container_width=True)
    
    st.markdown("---")
    st.info("**Note:** Chaque page contient environ 20-30 annonces.")
    
    # Section informations
    with st.expander("‚ÑπÔ∏è Informations"):
        st.markdown("""
        **Cat√©gories disponibles:**
        1. V√™tements Homme
        2. Chaussures Homme
        3. V√™tements Enfants
        4. Chaussures Enfants
        
        **Donn√©es collect√©es:**
        - Type d'article
        - Prix (CFA)
        - Adresse
        - Image
        - Date du scraping
        """)

# Contenu principal
if start_scraping:
    # URLs pour le scraping
    urls = {
        "V√™tements Homme": 'https://sn.coinafrique.com/categorie/vetements-homme',
        "Chaussures Homme": 'https://sn.coinafrique.com/categorie/chaussures-homme',
        "V√™tements Enfants": 'https://sn.coinafrique.com/categorie/vetements-enfants',
        "Chaussures Enfants": 'https://sn.coinafrique.com/categorie/chaussures-enfants'
    }
    
    # Initialiser les DataFrames
    dataframes = {}
    
    # Conteneur pour la progression
    progress_container = st.container()
    
    with progress_container:
        st.subheader("üìä Progression du scraping")
        progress_bar = st.progress(0, text="Pr√©paration...")
        
        # Scraping pour chaque cat√©gorie
        categories = list(urls.keys())
        for i, category in enumerate(categories):
            st.write(f"**{category}**...")
            
            # Barre de progression pour cette cat√©gorie
            category_progress = st.progress(0, text=f"Page 1/{pages}")
            
            # Scraping
            df = scraping(urls[category], pages, category_progress)
            dataframes[category] = df
            
            # Supprimer la barre de progression de la cat√©gorie
            category_progress.empty()
            
            # Mise √† jour de la barre principale
            progress_bar.progress((i + 1) / len(categories), 
                                 text=f"{i + 1}/{len(categories)} cat√©gories termin√©es")
        
        progress_bar.empty()
        st.success("‚úÖ Scraping termin√© avec succ√®s !")
    
    # Section des statistiques
    st.subheader("üìà Statistiques des donn√©es collect√©es")
    
    cols = st.columns(4)
    for idx, (category, df) in enumerate(dataframes.items()):
        with cols[idx % 4]:
            st.metric(
                label=category,
                value=f"{len(df)} annonces",
                delta=f"{len(df)//pages} annonces/page" if pages > 0 else "0"
            )
    
    # Section d'affichage des donn√©es
    st.subheader("üëÅÔ∏è Aper√ßu des donn√©es")
    
    # S√©lecteur de cat√©gorie pour l'aper√ßu
    selected_category = st.selectbox(
        "Choisir une cat√©gorie √† afficher:",
        list(dataframes.keys())
    )
    
    if selected_category in dataframes:
        df_display = dataframes[selected_category]
        st.dataframe(
            df_display.head(10),
            use_container_width=True,
            column_config={
                "image": st.column_config.ImageColumn("Image", width="small"),
                "prix": st.column_config.NumberColumn("Prix (CFA)", format="%d CFA"),
                "type": st.column_config.TextColumn("Type d'article", width="medium"),
                "adresse": st.column_config.TextColumn("Adresse", width="medium"),
                "date_scraping": st.column_config.DatetimeColumn("Date de scraping")
            }
        )
        
        # Afficher quelques images
        if not df_display.empty and 'image' in df_display.columns:
            st.subheader("üñºÔ∏è Quelques images des annonces")
            image_urls = df_display['image'].dropna().head(6).tolist()
            if image_urls:
                cols = st.columns(3)
                for idx, img_url in enumerate(image_urls[:6]):
                    with cols[idx % 3]:
                        st.image(img_url, caption=f"Annonce {idx+1}", use_column_width=True)
    
    # Section de t√©l√©chargement
    st.subheader("üì• T√©l√©chargement des donn√©es")
    
    # Cr√©er un dossier data s'il n'existe pas
    data_dir = "data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # Enregistrer les fichiers CSV
    st.write("T√©l√©chargez les donn√©es compl√®tes au format CSV:")
    
    download_cols = st.columns(4)
    for idx, (category, df) in enumerate(dataframes.items()):
        with download_cols[idx % 4]:
            # G√©n√©rer le nom du fichier
            filename = f"{category.lower().replace(' ', '_')}.csv"
            filepath = os.path.join(data_dir, filename)
            
            # Sauvegarder le fichier
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            # Cr√©er le lien de t√©l√©chargement
            st.markdown(get_csv_download_link(df, filename, f"üì• {category}"), 
                       unsafe_allow_html=True)
            
            # Afficher des infos suppl√©mentaires
            st.caption(f"{len(df)} annonces")
    
    # Option pour t√©l√©charger toutes les donn√©es en un seul fichier Excel
    st.markdown("---")
    st.subheader("üì¶ Option avanc√©e")
    
    if st.button("üìä G√©n√©rer un fichier Excel avec toutes les donn√©es", 
                use_container_width=True):
        with st.spinner("G√©n√©ration du fichier Excel..."):
            excel_path = os.path.join(data_dir, "toutes_donnees_coinafrique.xlsx")
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                for category, df in dataframes.items():
                    # Nettoyer le nom de la feuille
                    sheet_name = category[:31]  # Excel limite √† 31 caract√®res
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            # Lire le fichier Excel pour le t√©l√©chargement
            with open(excel_path, "rb") as f:
                excel_data = f.read()
            
            b64 = base64.b64encode(excel_data).decode()
            href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" \
                    download="toutes_donnees_coinafrique.xlsx" \
                    style="display: inline-block;\
                           padding: 0.75rem 1.5rem;\
                           background-color: #2196F3;\
                           color: white;\
                           text-decoration: none;\
                           border-radius: 5px;\
                           font-weight: bold;\
                           font-size: 1.1em;">üìä T√©l√©charger le fichier Excel complet</a>'
            
            st.markdown(href, unsafe_allow_html=True)
            st.success("Fichier Excel g√©n√©r√© avec succ√®s !")

else:
    # Page d'accueil
    st.markdown("""
    ## üìã Bienvenue sur CoinAfrique Scraper
    
    Cette application vous permet de:
    
    1. **Scraper les donn√©es** depuis CoinAfrique S√©n√©gal
    2. **Visualiser les annonces** en temps r√©el
    3. **T√©l√©charger les donn√©es** au format CSV ou Excel
    
    ### üéØ Cat√©gories disponibles:
    - üëï V√™tements pour Hommes
    - üëû Chaussures pour Hommes
    - üë∂ V√™tements pour Enfants
    - üëü Chaussures pour Enfants
    
    ### üöÄ Comment utiliser:
    1. Configurez le nombre de pages dans la barre lat√©rale
    2. Cliquez sur "Lancer le scraping"
    3. Visualisez les donn√©es collect√©es
    4. T√©l√©chargez les fichiers CSV ou Excel
    
    ---
    
    **üí° Conseil:** Commencez avec 2-3 pages pour tester, puis augmentez selon vos besoins.
    """)
    
    # Exemple de structure de donn√©es
    with st.expander("üëÅÔ∏è Exemple de donn√©es collect√©es"):
        example_data = pd.DataFrame({
            "type": ["Chemise homme", "Baskets Nike", "Robe enfant"],
            "prix": [5000, 25000, 3500],
            "adresse": ["Dakar", "Thi√®s", "Mbour"],
            "image": [
                "https://example.com/image1.jpg",
                "https://example.com/image2.jpg",
                "https://example.com/image3.jpg"
            ],
            "date_scraping": ["2024-01-06 10:30:00"] * 3
        })
        st.dataframe(example_data, use_container_width=True)

# Pied de page
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.9em;'>
    Application d√©velopp√©e avec ‚ù§Ô∏è en utilisant Streamlit | 
    Donn√©es provenant de <a href='https://sn.coinafrique.com' target='_blank'>CoinAfrique S√©n√©gal</a>
    </div>
    """,
    unsafe_allow_html=True
)
